{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1b1f468",
   "metadata": {},
   "source": [
    "# BindingDB Dataset Exploration for Protein-Ligand Binding Affinity Prediction\n",
    "\n",
    "This notebook explores the BindingDB dataset to prepare data for pose-free protein-ligand binding affinity prediction. We'll focus on combining protein structure information with SMILES molecular representations for a lightweight AI approach to drug discovery.\n",
    "\n",
    "## Project Goals:\n",
    "- **Target**: Ligand-Protein Binding Affinity Estimation (Mini AlphaFold Challenge)\n",
    "- **Approach**: Pose-free prediction using protein sequences/structures + SMILES codes\n",
    "- **Dataset**: BindingDB - binding affinities for protein-ligand pairs\n",
    "- **Model Constraint**: ≤50M parameters for hackathon feasibility"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b28677",
   "metadata": {},
   "source": [
    "## 1. Install and Import Required Libraries\n",
    "\n",
    "We'll install and import the necessary libraries for data processing, molecular handling, and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0934a480",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "# Uncomment and run these lines if packages are not installed\n",
    "# !pip install pandas numpy matplotlib seaborn\n",
    "# !pip install rdkit-pypi  # For SMILES processing\n",
    "# !pip install biopython  # For protein sequence handling\n",
    "# !pip install requests   # For downloading data\n",
    "# !pip install scikit-learn  # For ML utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d917f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import standard libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import requests\n",
    "import gzip\n",
    "import io\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import molecular handling libraries\n",
    "try:\n",
    "    from rdkit import Chem\n",
    "    from rdkit.Chem import Descriptors, rdMolDescriptors\n",
    "    from rdkit.Chem import AllChem\n",
    "    print(\"✓ RDKit imported successfully\")\n",
    "except ImportError:\n",
    "    print(\"❌ RDKit not available. Install with: pip install rdkit-pypi\")\n",
    "\n",
    "# Import bioinformatics libraries\n",
    "try:\n",
    "    from Bio.SeqUtils.ProtParam import ProteinAnalysis\n",
    "    from Bio.SeqUtils import molecular_weight\n",
    "    print(\"✓ BioPython imported successfully\")\n",
    "except ImportError:\n",
    "    print(\"❌ BioPython not available. Install with: pip install biopython\")\n",
    "\n",
    "# Import ML libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "print(\"✓ Core libraries imported successfully\")\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0721372",
   "metadata": {},
   "source": [
    "## 2. Download and Load BindingDB Dataset\n",
    "\n",
    "BindingDB is a comprehensive database of measured binding affinities for protein-ligand pairs. We'll download a subset of the data focusing on entries with both protein structure information and SMILES codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56579c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data directory\n",
    "data_dir = Path(\"data\")\n",
    "data_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# BindingDB download URL (TSV format with filtered data)\n",
    "# This URL provides a subset of BindingDB with Ki, Kd, and IC50 values\n",
    "bindingdb_url = \"https://www.bindingdb.org/bind/downloads/BindingDB_All_202310_tsv.zip\"\n",
    "\n",
    "# For demo purposes, we'll use a smaller subset or create sample data\n",
    "# In practice, you'd download the full dataset\n",
    "\n",
    "def download_bindingdb_sample():\n",
    "    \"\"\"\n",
    "    Download a sample of BindingDB data for exploration\n",
    "    Note: The full dataset is very large (~2GB), so we'll start with a subset\n",
    "    \"\"\"\n",
    "    print(\"Creating sample BindingDB data for exploration...\")\n",
    "    \n",
    "    # Sample data structure based on BindingDB format\n",
    "    sample_data = {\n",
    "        'Target Name': ['Human albumin', 'HIV-1 protease', 'Thrombin', 'Acetylcholinesterase', 'Carbonic anhydrase II'] * 200,\n",
    "        'UniProt (SwissProt) Primary ID of Target Chain': ['P02768', 'P03366', 'P00734', 'P22303', 'P00918'] * 200,\n",
    "        'PDB ID(s) for Ligand-Target Complex': ['4F5S', '1HPV', '1PPE', '1ACJ', '2CBA'] * 200,\n",
    "        'Ligand SMILES': [\n",
    "            'CC(C)CC1=CC=C(C=C1)C(C)C(=O)O',  # Ibuprofen-like\n",
    "            'CC(C)(C)NC(=O)C1=CC=CC=C1',        # Simple amide\n",
    "            'CCN(CC)CCOC(=O)C1=CC=CC=C1',      # Ester compound\n",
    "            'CC1=CC=C(C=C1)S(=O)(=O)N',        # Sulfonamide\n",
    "            'CC(=O)NC1=CC=C(C=C1)O'            # Acetaminophen-like\n",
    "        ] * 200,\n",
    "        'Ki (nM)': np.random.lognormal(5, 2, 1000),  # Log-normal distribution typical for Ki values\n",
    "        'Kd (nM)': np.random.lognormal(4.5, 1.8, 1000),\n",
    "        'IC50 (nM)': np.random.lognormal(5.5, 2.2, 1000),\n",
    "        'Target Source Organism According to Curator or DataSource': ['Homo sapiens'] * 1000,\n",
    "        'Ligand INCHI Key': ['INCHIKEY-' + str(i).zfill(6) for i in range(1000)],\n",
    "        'Number of Protein Chains in Target (>1 implies a multichain complex)': [1] * 800 + [2] * 150 + [4] * 50,\n",
    "        'Molecular Weight of Ligand (g/mol)': np.random.normal(350, 150, 1000).clip(50, 800)\n",
    "    }\n",
    "    \n",
    "    df = pd.DataFrame(sample_data)\n",
    "    \n",
    "    # Add some missing values to simulate real data\n",
    "    df.loc[np.random.choice(df.index, 100, replace=False), 'Ki (nM)'] = np.nan\n",
    "    df.loc[np.random.choice(df.index, 120, replace=False), 'Kd (nM)'] = np.nan\n",
    "    df.loc[np.random.choice(df.index, 80, replace=False), 'IC50 (nM)'] = np.nan\n",
    "    df.loc[np.random.choice(df.index, 50, replace=False), 'PDB ID(s) for Ligand-Target Complex'] = ''\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Load the sample dataset\n",
    "print(\"Loading BindingDB sample data...\")\n",
    "df_bindingdb = download_bindingdb_sample()\n",
    "\n",
    "print(f\"✓ Loaded sample dataset with {len(df_bindingdb)} entries\")\n",
    "print(f\"Dataset shape: {df_bindingdb.shape}\")\n",
    "print(\"\\nColumn names:\")\n",
    "for i, col in enumerate(df_bindingdb.columns, 1):\n",
    "    print(f\"{i:2d}. {col}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33f3763",
   "metadata": {},
   "source": [
    "## 3. Data Exploration and Statistics\n",
    "\n",
    "Let's explore the dataset structure, distribution of binding affinities, and identify data quality patterns that will be important for our pose-free binding affinity prediction model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6abfcc72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic dataset information\n",
    "print(\"=== BindingDB Dataset Overview ===\")\n",
    "print(f\"Total entries: {len(df_bindingdb):,}\")\n",
    "print(f\"Total columns: {len(df_bindingdb.columns)}\")\n",
    "print()\n",
    "\n",
    "# Display first few rows\n",
    "print(\"=== Sample Data ===\")\n",
    "display(df_bindingdb.head())\n",
    "print()\n",
    "\n",
    "# Data types and missing values\n",
    "print(\"=== Data Quality Summary ===\")\n",
    "info_df = pd.DataFrame({\n",
    "    'Column': df_bindingdb.columns,\n",
    "    'Data Type': df_bindingdb.dtypes,\n",
    "    'Non-Null Count': df_bindingdb.count(),\n",
    "    'Missing Count': df_bindingdb.isnull().sum(),\n",
    "    'Missing %': (df_bindingdb.isnull().sum() / len(df_bindingdb) * 100).round(2)\n",
    "})\n",
    "display(info_df)\n",
    "\n",
    "# Unique proteins and ligands\n",
    "print(\"\\n=== Dataset Diversity ===\")\n",
    "print(f\"Unique target proteins: {df_bindingdb['Target Name'].nunique()}\")\n",
    "print(f\"Unique UniProt IDs: {df_bindingdb['UniProt (SwissProt) Primary ID of Target Chain'].nunique()}\")\n",
    "print(f\"Unique PDB structures: {df_bindingdb[df_bindingdb['PDB ID(s) for Ligand-Target Complex'] != '']['PDB ID(s) for Ligand-Target Complex'].nunique()}\")\n",
    "print(f\"Unique SMILES: {df_bindingdb['Ligand SMILES'].nunique()}\")\n",
    "\n",
    "# Target protein distribution\n",
    "print(\"\\n=== Top 10 Most Studied Proteins ===\")\n",
    "target_counts = df_bindingdb['Target Name'].value_counts().head(10)\n",
    "print(target_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2a5fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize binding affinity distributions\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('Binding Affinity Distributions in BindingDB Sample', fontsize=16)\n",
    "\n",
    "# Ki distribution\n",
    "ki_data = df_bindingdb['Ki (nM)'].dropna()\n",
    "axes[0, 0].hist(np.log10(ki_data), bins=30, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "axes[0, 0].set_xlabel('log10(Ki) [nM]')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "axes[0, 0].set_title(f'Ki Distribution (n={len(ki_data)})')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Kd distribution\n",
    "kd_data = df_bindingdb['Kd (nM)'].dropna()\n",
    "axes[0, 1].hist(np.log10(kd_data), bins=30, alpha=0.7, color='lightcoral', edgecolor='black')\n",
    "axes[0, 1].set_xlabel('log10(Kd) [nM]')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "axes[0, 1].set_title(f'Kd Distribution (n={len(kd_data)})')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# IC50 distribution\n",
    "ic50_data = df_bindingdb['IC50 (nM)'].dropna()\n",
    "axes[1, 0].hist(np.log10(ic50_data), bins=30, alpha=0.7, color='lightgreen', edgecolor='black')\n",
    "axes[1, 0].set_xlabel('log10(IC50) [nM]')\n",
    "axes[1, 0].set_ylabel('Frequency')\n",
    "axes[1, 0].set_title(f'IC50 Distribution (n={len(ic50_data)})')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Molecular weight distribution\n",
    "mw_data = df_bindingdb['Molecular Weight of Ligand (g/mol)'].dropna()\n",
    "axes[1, 1].hist(mw_data, bins=30, alpha=0.7, color='gold', edgecolor='black')\n",
    "axes[1, 1].set_xlabel('Molecular Weight [g/mol]')\n",
    "axes[1, 1].set_ylabel('Frequency')\n",
    "axes[1, 1].set_title(f'Ligand MW Distribution (n={len(mw_data)})')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary statistics\n",
    "print(\"=== Binding Affinity Summary Statistics ===\")\n",
    "affinity_cols = ['Ki (nM)', 'Kd (nM)', 'IC50 (nM)']\n",
    "summary_stats = df_bindingdb[affinity_cols].describe()\n",
    "print(summary_stats)\n",
    "\n",
    "# Data availability for each affinity type\n",
    "print(\"\\n=== Data Availability ===\")\n",
    "for col in affinity_cols:\n",
    "    available = df_bindingdb[col].notna().sum()\n",
    "    total = len(df_bindingdb)\n",
    "    print(f\"{col}: {available:,}/{total:,} ({available/total*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01d13ca",
   "metadata": {},
   "source": [
    "## 4. Filter Data for Binding Affinity Tasks\n",
    "\n",
    "Now we'll filter the dataset to create a high-quality subset suitable for machine learning. We'll focus on entries with valid SMILES codes, protein structure information, and reliable binding affinity measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a59fda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define filtering criteria for high-quality data\n",
    "def filter_bindingdb_data(df):\n",
    "    \"\"\"\n",
    "    Filter BindingDB data for machine learning tasks\n",
    "    \"\"\"\n",
    "    print(\"=== Data Filtering Pipeline ===\")\n",
    "    print(f\"Starting with {len(df):,} entries\")\n",
    "    \n",
    "    # Step 1: Remove entries without SMILES\n",
    "    df_filtered = df[df['Ligand SMILES'].notna() & (df['Ligand SMILES'] != '')].copy()\n",
    "    print(f\"After removing entries without SMILES: {len(df_filtered):,} entries\")\n",
    "    \n",
    "    # Step 2: Remove entries without UniProt ID (protein identifier)\n",
    "    df_filtered = df_filtered[df_filtered['UniProt (SwissProt) Primary ID of Target Chain'].notna()].copy()\n",
    "    print(f\"After removing entries without UniProt ID: {len(df_filtered):,} entries\")\n",
    "    \n",
    "    # Step 3: Keep entries with at least one binding affinity measurement\n",
    "    has_affinity = (df_filtered['Ki (nM)'].notna() | \n",
    "                   df_filtered['Kd (nM)'].notna() | \n",
    "                   df_filtered['IC50 (nM)'].notna())\n",
    "    df_filtered = df_filtered[has_affinity].copy()\n",
    "    print(f\"After requiring at least one affinity measurement: {len(df_filtered):,} entries\")\n",
    "    \n",
    "    # Step 4: Filter by molecular weight (typical drug-like range)\n",
    "    mw_filter = (df_filtered['Molecular Weight of Ligand (g/mol)'] >= 100) & \\\n",
    "                (df_filtered['Molecular Weight of Ligand (g/mol)'] <= 1000)\n",
    "    df_filtered = df_filtered[mw_filter].copy()\n",
    "    print(f\"After molecular weight filter (100-1000 g/mol): {len(df_filtered):,} entries\")\n",
    "    \n",
    "    # Step 5: Remove extreme outliers in binding affinity\n",
    "    for col in ['Ki (nM)', 'Kd (nM)', 'IC50 (nM)']:\n",
    "        if col in df_filtered.columns:\n",
    "            # Remove values outside reasonable range (0.1 nM to 100 μM = 100,000 nM)\n",
    "            valid_range = (df_filtered[col] >= 0.1) & (df_filtered[col] <= 100000)\n",
    "            df_filtered.loc[~valid_range, col] = np.nan\n",
    "    \n",
    "    # Step 6: Focus on human proteins for this demo\n",
    "    df_filtered = df_filtered[df_filtered['Target Source Organism According to Curator or DataSource'] == 'Homo sapiens'].copy()\n",
    "    print(f\"After filtering for human proteins: {len(df_filtered):,} entries\")\n",
    "    \n",
    "    return df_filtered\n",
    "\n",
    "# Apply filtering\n",
    "df_clean = filter_bindingdb_data(df_bindingdb)\n",
    "\n",
    "print(f\"\\n=== Final Dataset Summary ===\")\n",
    "print(f\"Clean dataset size: {len(df_clean):,} entries\")\n",
    "print(f\"Unique proteins: {df_clean['Target Name'].nunique()}\")\n",
    "print(f\"Unique ligands: {df_clean['Ligand SMILES'].nunique()}\")\n",
    "\n",
    "# Check data availability after filtering\n",
    "print(\"\\n=== Affinity Data Availability (Clean Dataset) ===\")\n",
    "for col in ['Ki (nM)', 'Kd (nM)', 'IC50 (nM)']:\n",
    "    available = df_clean[col].notna().sum()\n",
    "    total = len(df_clean)\n",
    "    print(f\"{col}: {available:,}/{total:,} ({available/total*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7d7649",
   "metadata": {},
   "source": [
    "## 5. Protein Structure Data Processing\n",
    "\n",
    "For pose-free binding affinity prediction, we need to extract meaningful features from protein sequences and structures. We'll create protein representations that can be used without requiring 3D coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ba6b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample protein sequences for demonstration\n",
    "# In practice, you would fetch these from UniProt or PDB\n",
    "def create_sample_protein_features(df):\n",
    "    \"\"\"\n",
    "    Create sample protein features for demonstration\n",
    "    In a real implementation, you would:\n",
    "    1. Fetch protein sequences from UniProt using the UniProt IDs\n",
    "    2. Extract features from the sequences using BioPython\n",
    "    3. Optionally use pre-trained protein embeddings (ESM-2, ProtBERT)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Sample protein sequences (shortened for demo)\n",
    "    protein_sequences = {\n",
    "        'P02768': 'MKWVTFISLLFLFSSAYSRGVFRRDAHKSEVAHRFKDLGEENFKALVLIAFAQYLQQCPFEDHVKLVNEVTEFAKTCVADESAENCDKSLHTLFGDKLCTVATLRETYGEMADCCAKQEPERNECFLQHKDDNPNLPRLVRPEVDVMCTAFHDNEETFLKKYLYEIARRHPYFYAPELLFFAKRYKAAFTECCQAADKAACLLPKLDELRDEGKASSAKQRLKCASLQKFGERAFKAWAVARLSQRFPKAEFAEVSKLVTDLTKVHTECCHGDLLECADDRADLAKYICENQDSISSKLKECCEKPLLEKSHCIAEVENDEMPADLPSLAADFVESKDVCKNYAEAKDVFLGMFLYEYARRHPDYSVVLLLRLAKTYETTLEKCCAAADPHECYAKVFDEFKPLVEEPQNLIKQNCELFEQLGEYKFQNALLVRYTKKVPQVSTPTLVEVSRNLGKVGSKCCKHPEAKRMPCAEDYLSVVLNQLCVLHEKTPVSDRVTKCCTESLVNRRPCFSALEVDETYVPKEFNAETFTFHADICTLSEKERQIKKQTALVELVKHKPKATKEQLKAVMDDFAAFVEKCCKADDKETCFAEEGKKLVAASQAALGL',\n",
    "        'P03366': 'PQITLWQRPLVTIKIGGQLKEALLDTGADDTVLEEMSLPGRWKPKMIGGIGGFIKVRQYDQILIEICGHKAIGTVLVGPTPVNIIGRNLLTQIGCTLNFPISPIETVPVKLKPGMDGPKVKQWPLTEEKIKALVEICTEMEKEGKISKIGPENPYNTPVFAIKKKDSTKWRKLVDFRELNKRTQDFWEVQLGIPHPAGLKKKKSVTVLDVGDAYFSVPLDEDFRKYTAFTIPSINNETPGIRYQYNVLPQGWKGSPAIFQSSMTKILEPFRKQNPDIVIYQYMDDLYVGSDLEIGQHRTKIEELRQHLLRWGLTTPDKKHQKEPPFLWMGYELHPDKWTVQPIVLPEKDSWTVNDIQKLVGKLNWASQIYPGIKVRQLCKLLRGTKALTEVIPLTEEAELELAENREILKEPVHGVYYDPSKDLIAEIQKQGQGQWTYQIYQEPFKNLKTGKYARMRGAHTNDVKQLTEAVQKITTESIVIWGKTPKFKLPIQKETWETWWTEYWQATWIPEWEFVNTPPLVKLWYQLEKEPIVGAETFYVDGAANRETKLGKAGYVTNRGRQKVVTLTDTTNQKTELQAIYLALQDSGLEVNIVTDSQYALGIIQAQPDQSESELVNQIIEQLINKEKVYLAWVPAHKGIGGNEQVDKLVSAGIRKVLFLDGIDKAQEEHEKYHSNWRAMASDFNLPPVVAKEIVASCDKCQLKGEAMHGQVDCSPGIWQLDCTHLEGKVILVAVHVASGYIEAEVIPAETGQETAYFLLKLAGRWPVKTIHTDNGSNFTGATVRAACWWAGIKQEFGIPYNPQSQGVVESMNKELKKIIGQVRDQAEHLKTAVQMAVFIHNFKRKGGIGGYSAGERIVDIIATDIQTKELQKQITKIQNFRVYYRDSRDPLWKGPAKLLWKGEGAVVIQDNSDIKVVPRRKAKIIRDYGKQMAGDDCVASRQDED',\n",
    "        'P00734': 'MNKPLLLVAILLVLASLCHATFWQSLRQSHPDSTDHMKPLPWPKTLWQRPLVTIKIGGQLKEALLDTGADDTVLEEMSLPGRWKPKMIGGIGGFIKVRQYDQILIEICGHKAIGTVLVGPTPVNIIGRNLLTQIGCTLNFPISPIETVPVKLKPGMDGPKVKQWPLTEEKIKALVEICTEMEKEGKISKIGPENPYNTPVFAIKKKDSTKWRKLVDFRELNKRTQDFWEVQLGIPHPAGLKKKKSVTVLDVGDAYFSVPLDEDFRKYTAFTIPSINNETPGIRYQYNVLPQGWKGSPAIFQSSMTKILEPFRKQNPDIVIYQYMDDLYVGSDLEIGQHRTKIEELRQHLLRWGLTTPDKKHQKEPPFLWMGYELHPDKWTVQPIVLPEKDSWTVNDIQKLVGKLNWASQIYPGIKVRQLCKLLRGTKALTEVIPLTEEAELELAENREILKEPVHGVYYDPSKDLIAEIQKQGQGQWTYQIYQEPFKNLKTGKYARMRGAHTNDVKQLTEAVQKITTESIVIWGKTPKFKLPIQKETWETWWTEYWQATWIPEWEFVNTPPLVKLWYQLEKEPIVGAETFYVDGAANRETKLGKAGYVTNRGRQKVVTLTDTTNQKTELQAIYLALQDSGLEVNIVTDSQYALGIIQAQPDQSESELVNQIIEQLINKEKVYLAWVPAHKGIGGNEQVDKLVSAGIRKVLFLDGIDKAQEEHEKYHSNWRAMASDFNLPPVVAKEIVASCDKCQLKGEAMHGQVDCSPGIWQLDCTHLEGKVILVAVHVASGYIEAEVIPAETGQETAYFLLKLAGRWPVKTIHTDNGSNFTGATVRAACWWAGIKQEFGIPYNPQSQGVVESMNKELKKIIGQVRDQAEHLKTAVQMAVFIHNFKRKGGIGGYSAGERIVDIIATDIQTKELQKQITKIQNFRVYYRDSRDPLWKGPAKLLWKGEGAVVIQDNSDIKVVPRRKAKIIRDYGKQMAGDDCVASRQDED',\n",
    "        'P22303': 'MELFRPHFLLIRCLPLALLCLLPSLALAEDRSLLPKLHYFNARGRQVGLNLTGGGSVGAPLVLAVALLIAAALLPSEQARAAGRRLRRLLLAALLLGAGGGGLLLLLLLLLAALLLLLLGGGGGGLLLLAALLLLAAALLLLAAGGGGGLLLLAALLLLAAAALLLGGGGGGLLLLAALLLLAAAALLLGGGGGGLLLLAALLLLAAAALLLGGGGGGLLLLAALLLLAAAALLLGGGGGGLLLLAALLLLAAAALLLGGGGGGLLLLAALLLLAAAALLLGGGGGGLLLLAALLLLAAAALLLGGGGGGLLLLAALLLLAAAALLLGGGGGGLLLLAALLLLAAAALLLGGGGGGLLLLAALLLLAAAALLLGGGGGGLLLLAALLLLAAAALLLGGGGGGLLLLAALLLLAAAALLLGGGGGGLLLLAALLLLAAAALLLGGGGGGLLLLAALLLLAAAALLLGGGGGGLLLLAALLLLAAAALLLGGGGGGLLLLAALLLLAAAALLLGGGGGGLLLLAALLLLAAAALLLGGGGGGLLLLAALLLLAAAALLLGGGGGGLLLLAALLLLAAAALLLGGGGGGLLLLAALLLLAAAALLLGGGGGGLLLLAALLLLAAAALLLGGGGGGLLLLAALLLLAAAALLLGGGGGGLLLLAALLLLAAAALLLGGGGGGLLLLAALLLLAAAALLLGGGGGGLLLLAALLLLAAAALLLGGGGGGLLLLAALLLLAAAALLLGGGGGGLLLLAALLLLAAAALLL',\n",
    "        'P00918': 'MSHHWGYGKHNGPEHWHKDFPIAKGERQSPVDIDTHTAKYDPSLKPLSVSYDQATSLRILNNGHAFNVEFDDSQDKAVLKGGPLDGTYRLIQFHFHWGSLDGQGSEHTVDKKKYAAELHLVHWNTKYGDFGKAVQQPDGLAVLGIFLKVGSAKPGLQKVVDVLDSIKTKGKSADFTNFDPRGLLPESLDYWTYPGSLTTPPLLECVTWIVLKEPISVSSEQVLKFRKLNFNGEGEPEELMVDNWRPAQPLKNRQIKASFK'\n",
    "    }\n",
    "    \n",
    "    protein_features = []\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        uniprot_id = row['UniProt (SwissProt) Primary ID of Target Chain']\n",
    "        \n",
    "        # Get sequence (in practice, fetch from UniProt)\n",
    "        sequence = protein_sequences.get(uniprot_id, protein_sequences['P02768'])  # Default to albumin\n",
    "        \n",
    "        if len(sequence) > 500:  # Truncate very long sequences for demo\n",
    "            sequence = sequence[:500]\n",
    "        \n",
    "        try:\n",
    "            # Calculate basic protein features using BioPython\n",
    "            analysis = ProteinAnalysis(sequence)\n",
    "            \n",
    "            features = {\n",
    "                'uniprot_id': uniprot_id,\n",
    "                'sequence_length': len(sequence),\n",
    "                'molecular_weight': analysis.molecular_weight(),\n",
    "                'aromaticity': analysis.aromaticity(),\n",
    "                'instability_index': analysis.instability_index(),\n",
    "                'isoelectric_point': analysis.isoelectric_point(),\n",
    "                'gravy': analysis.gravy(),  # Grand average of hydropathy\n",
    "            }\n",
    "            \n",
    "            # Add amino acid composition\n",
    "            aa_percent = analysis.get_amino_acids_percent()\n",
    "            for aa, percent in aa_percent.items():\n",
    "                features[f'aa_{aa}_percent'] = percent\n",
    "            \n",
    "            # Add secondary structure fraction\n",
    "            sec_struct = analysis.secondary_structure_fraction()\n",
    "            features['helix_fraction'] = sec_struct[0]\n",
    "            features['turn_fraction'] = sec_struct[1] \n",
    "            features['sheet_fraction'] = sec_struct[2]\n",
    "            \n",
    "            protein_features.append(features)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing protein {uniprot_id}: {e}\")\n",
    "            # Add default features for failed cases\n",
    "            protein_features.append({\n",
    "                'uniprot_id': uniprot_id,\n",
    "                'sequence_length': 300,\n",
    "                'molecular_weight': 35000,\n",
    "                'aromaticity': 0.1,\n",
    "                'instability_index': 40,\n",
    "                'isoelectric_point': 7.0,\n",
    "                'gravy': -0.5,\n",
    "                'helix_fraction': 0.3,\n",
    "                'turn_fraction': 0.3,\n",
    "                'sheet_fraction': 0.4\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(protein_features)\n",
    "\n",
    "# Extract protein features\n",
    "print(\"Extracting protein features...\")\n",
    "protein_features_df = create_sample_protein_features(df_clean.head(100))  # Process first 100 for demo\n",
    "\n",
    "print(f\"✓ Extracted features for {len(protein_features_df)} proteins\")\n",
    "print(\"\\nProtein features shape:\", protein_features_df.shape)\n",
    "print(\"\\nSample protein features:\")\n",
    "display(protein_features_df.head())\n",
    "\n",
    "# Visualize protein feature distributions\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "fig.suptitle('Protein Feature Distributions', fontsize=16)\n",
    "\n",
    "# Key protein features to visualize\n",
    "features_to_plot = [\n",
    "    ('sequence_length', 'Sequence Length'),\n",
    "    ('molecular_weight', 'Molecular Weight (Da)'),\n",
    "    ('isoelectric_point', 'Isoelectric Point'),\n",
    "    ('aromaticity', 'Aromaticity'),\n",
    "    ('gravy', 'GRAVY (Hydropathy)'),\n",
    "    ('instability_index', 'Instability Index')\n",
    "]\n",
    "\n",
    "for i, (feature, title) in enumerate(features_to_plot):\n",
    "    row, col = i // 3, i % 3\n",
    "    axes[row, col].hist(protein_features_df[feature], bins=20, alpha=0.7, edgecolor='black')\n",
    "    axes[row, col].set_title(title)\n",
    "    axes[row, col].set_xlabel(feature.replace('_', ' ').title())\n",
    "    axes[row, col].set_ylabel('Frequency')\n",
    "    axes[row, col].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44456416",
   "metadata": {},
   "source": [
    "## 6. SMILES Molecular Representation Processing\n",
    "\n",
    "SMILES (Simplified Molecular Input Line Entry System) codes represent the chemical structure of molecules as strings. We'll process these to extract molecular features and fingerprints for our pose-free binding prediction model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574cef39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process SMILES strings and extract molecular features\n",
    "def process_smiles_features(df, max_molecules=100):\n",
    "    \"\"\"\n",
    "    Process SMILES strings and extract molecular descriptors using RDKit\n",
    "    \"\"\"\n",
    "    print(\"Processing SMILES strings...\")\n",
    "    \n",
    "    molecular_features = []\n",
    "    valid_smiles = []\n",
    "    \n",
    "    for idx, smiles in enumerate(df['Ligand SMILES'].head(max_molecules)):\n",
    "        if pd.isna(smiles) or smiles == '':\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            # Parse SMILES\n",
    "            mol = Chem.MolFromSmiles(smiles)\n",
    "            if mol is None:\n",
    "                continue\n",
    "                \n",
    "            # Calculate molecular descriptors\n",
    "            features = {\n",
    "                'smiles': smiles,\n",
    "                'mol_weight': Descriptors.MolWt(mol),\n",
    "                'logp': Descriptors.MolLogP(mol),\n",
    "                'num_h_donors': Descriptors.NumHDonors(mol),\n",
    "                'num_h_acceptors': Descriptors.NumHAcceptors(mol),\n",
    "                'num_rotatable_bonds': Descriptors.NumRotatableBonds(mol),\n",
    "                'tpsa': Descriptors.TPSA(mol),  # Topological polar surface area\n",
    "                'num_aromatic_rings': Descriptors.NumAromaticRings(mol),\n",
    "                'num_aliphatic_rings': Descriptors.NumAliphaticRings(mol),\n",
    "                'num_heteroatoms': Descriptors.NumHeteroatoms(mol),\n",
    "                'num_heavy_atoms': Descriptors.HeavyAtomCount(mol),\n",
    "                'fraction_csp3': Descriptors.FractionCsp3(mol),\n",
    "                'bertz_ct': Descriptors.BertzCT(mol),  # Complexity\n",
    "            }\n",
    "            \n",
    "            # Calculate molecular fingerprints (Morgan/ECFP)\n",
    "            fingerprint = AllChem.GetMorganFingerprintAsBitVect(mol, radius=2, nBits=1024)\n",
    "            fp_array = np.array(fingerprint)\n",
    "            \n",
    "            # Add fingerprint features\n",
    "            for i in range(min(50, len(fp_array))):  # Use first 50 bits for demo\n",
    "                features[f'fp_bit_{i}'] = fp_array[i]\n",
    "            \n",
    "            molecular_features.append(features)\n",
    "            valid_smiles.append(smiles)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing SMILES {smiles}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"✓ Successfully processed {len(molecular_features)} molecules\")\n",
    "    return pd.DataFrame(molecular_features)\n",
    "\n",
    "# Process molecular features\n",
    "if 'Chem' in globals():\n",
    "    mol_features_df = process_smiles_features(df_clean)\n",
    "    \n",
    "    print(f\"Molecular features shape: {mol_features_df.shape}\")\n",
    "    print(\"\\nSample molecular features:\")\n",
    "    display(mol_features_df[['smiles', 'mol_weight', 'logp', 'num_h_donors', 'num_h_acceptors', 'tpsa']].head())\n",
    "    \n",
    "    # Visualize molecular property distributions\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    fig.suptitle('Molecular Property Distributions', fontsize=16)\n",
    "    \n",
    "    # Key molecular properties\n",
    "    properties_to_plot = [\n",
    "        ('mol_weight', 'Molecular Weight (Da)'),\n",
    "        ('logp', 'LogP (Lipophilicity)'),\n",
    "        ('num_h_donors', 'H-Bond Donors'),\n",
    "        ('num_h_acceptors', 'H-Bond Acceptors'),\n",
    "        ('tpsa', 'TPSA (Ų)'),\n",
    "        ('num_rotatable_bonds', 'Rotatable Bonds')\n",
    "    ]\n",
    "    \n",
    "    for i, (prop, title) in enumerate(properties_to_plot):\n",
    "        row, col = i // 3, i % 3\n",
    "        axes[row, col].hist(mol_features_df[prop], bins=20, alpha=0.7, edgecolor='black')\n",
    "        axes[row, col].set_title(title)\n",
    "        axes[row, col].set_xlabel(prop.replace('_', ' ').title())\n",
    "        axes[row, col].set_ylabel('Frequency')\n",
    "        axes[row, col].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Lipinski's Rule of Five analysis\n",
    "    print(\"\\n=== Lipinski's Rule of Five Analysis ===\")\n",
    "    lipinski_violations = 0\n",
    "    total_molecules = len(mol_features_df)\n",
    "    \n",
    "    rule_violations = {\n",
    "        'MW > 500': (mol_features_df['mol_weight'] > 500).sum(),\n",
    "        'LogP > 5': (mol_features_df['logp'] > 5).sum(),\n",
    "        'H-donors > 5': (mol_features_df['num_h_donors'] > 5).sum(),\n",
    "        'H-acceptors > 10': (mol_features_df['num_h_acceptors'] > 10).sum()\n",
    "    }\n",
    "    \n",
    "    for rule, violations in rule_violations.items():\n",
    "        percentage = violations / total_molecules * 100\n",
    "        print(f\"{rule}: {violations}/{total_molecules} ({percentage:.1f}%)\")\n",
    "    \n",
    "    # Count molecules with 0, 1, 2+ violations\n",
    "    violations_per_mol = (\n",
    "        (mol_features_df['mol_weight'] > 500) +\n",
    "        (mol_features_df['logp'] > 5) +\n",
    "        (mol_features_df['num_h_donors'] > 5) +\n",
    "        (mol_features_df['num_h_acceptors'] > 10)\n",
    "    ).astype(int)\n",
    "    \n",
    "    print(f\"\\nMolecules with 0 violations (drug-like): {(violations_per_mol == 0).sum()}/{total_molecules} ({(violations_per_mol == 0).sum()/total_molecules*100:.1f}%)\")\n",
    "    print(f\"Molecules with 1 violation: {(violations_per_mol == 1).sum()}/{total_molecules} ({(violations_per_mol == 1).sum()/total_molecules*100:.1f}%)\")\n",
    "    print(f\"Molecules with 2+ violations: {(violations_per_mol >= 2).sum()}/{total_molecules} ({(violations_per_mol >= 2).sum()/total_molecules*100:.1f}%)\")\n",
    "    \n",
    "else:\n",
    "    print(\"❌ RDKit not available - skipping molecular feature extraction\")\n",
    "    print(\"Install RDKit with: pip install rdkit-pypi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d93e66f",
   "metadata": {},
   "source": [
    "## 7. Prepare Training/Validation Splits\n",
    "\n",
    "For reliable model evaluation, we need to create train/validation/test splits that avoid data leakage. We'll ensure that similar proteins or molecules don't appear across different splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3133ea99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare training/validation splits with no data leakage\n",
    "def create_train_val_splits(df, test_size=0.2, val_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Create train/val/test splits avoiding data leakage\n",
    "    Split by protein families to ensure no similar proteins across splits\n",
    "    \"\"\"\n",
    "    print(\"=== Creating Train/Validation/Test Splits ===\")\n",
    "    \n",
    "    # For demonstration, we'll split by target protein\n",
    "    # In practice, you might use protein family clustering\n",
    "    unique_proteins = df['UniProt (SwissProt) Primary ID of Target Chain'].unique()\n",
    "    \n",
    "    # Split proteins into train/val/test\n",
    "    proteins_train, proteins_temp = train_test_split(\n",
    "        unique_proteins, test_size=(test_size + val_size), random_state=random_state\n",
    "    )\n",
    "    \n",
    "    proteins_val, proteins_test = train_test_split(\n",
    "        proteins_temp, test_size=(test_size / (test_size + val_size)), random_state=random_state\n",
    "    )\n",
    "    \n",
    "    # Assign data points to splits based on protein\n",
    "    train_mask = df['UniProt (SwissProt) Primary ID of Target Chain'].isin(proteins_train)\n",
    "    val_mask = df['UniProt (SwissProt) Primary ID of Target Chain'].isin(proteins_val)\n",
    "    test_mask = df['UniProt (SwissProt) Primary ID of Target Chain'].isin(proteins_test)\n",
    "    \n",
    "    df_train = df[train_mask].copy()\n",
    "    df_val = df[val_mask].copy()\n",
    "    df_test = df[test_mask].copy()\n",
    "    \n",
    "    print(f\"Protein splits:\")\n",
    "    print(f\"  Train proteins: {len(proteins_train)} ({len(df_train)} data points)\")\n",
    "    print(f\"  Val proteins: {len(proteins_val)} ({len(df_val)} data points)\")\n",
    "    print(f\"  Test proteins: {len(proteins_test)} ({len(df_test)} data points)\")\n",
    "    \n",
    "    return df_train, df_val, df_test\n",
    "\n",
    "# Create splits\n",
    "df_train, df_val, df_test = create_train_val_splits(df_clean)\n",
    "\n",
    "# Create target variable (binding affinity)\n",
    "def create_target_variable(df):\n",
    "    \"\"\"\n",
    "    Create a unified binding affinity target variable\n",
    "    Priority: Ki > Kd > IC50\n",
    "    Convert to pKi/pKd/pIC50 (negative log10) for better model performance\n",
    "    \"\"\"\n",
    "    target = pd.Series(index=df.index, dtype=float)\n",
    "    \n",
    "    # Use Ki first (most direct measure)\n",
    "    ki_mask = df['Ki (nM)'].notna()\n",
    "    target[ki_mask] = -np.log10(df.loc[ki_mask, 'Ki (nM)'] * 1e-9)  # Convert to pKi\n",
    "    \n",
    "    # Use Kd if Ki not available\n",
    "    kd_mask = target.isna() & df['Kd (nM)'].notna()\n",
    "    target[kd_mask] = -np.log10(df.loc[kd_mask, 'Kd (nM)'] * 1e-9)  # Convert to pKd\n",
    "    \n",
    "    # Use IC50 if neither Ki nor Kd available\n",
    "    ic50_mask = target.isna() & df['IC50 (nM)'].notna()\n",
    "    target[ic50_mask] = -np.log10(df.loc[ic50_mask, 'IC50 (nM)'] * 1e-9)  # Convert to pIC50\n",
    "    \n",
    "    return target\n",
    "\n",
    "# Create target variables for each split\n",
    "y_train = create_target_variable(df_train)\n",
    "y_val = create_target_variable(df_val)\n",
    "y_test = create_target_variable(df_test)\n",
    "\n",
    "# Remove samples without target values\n",
    "train_valid_mask = y_train.notna()\n",
    "val_valid_mask = y_val.notna()\n",
    "test_valid_mask = y_test.notna()\n",
    "\n",
    "df_train_final = df_train[train_valid_mask].copy()\n",
    "df_val_final = df_val[val_valid_mask].copy()\n",
    "df_test_final = df_test[test_valid_mask].copy()\n",
    "\n",
    "y_train_final = y_train[train_valid_mask]\n",
    "y_val_final = y_val[val_valid_mask]\n",
    "y_test_final = y_test[test_valid_mask]\n",
    "\n",
    "print(f\"\\n=== Final Dataset Sizes ===\")\n",
    "print(f\"Train: {len(df_train_final)} samples\")\n",
    "print(f\"Validation: {len(df_val_final)} samples\")\n",
    "print(f\"Test: {len(df_test_final)} samples\")\n",
    "\n",
    "# Visualize target variable distribution\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "fig.suptitle('Binding Affinity Target Variable Distribution (pKi/pKd/pIC50)', fontsize=14)\n",
    "\n",
    "for i, (split_name, y_data) in enumerate([('Train', y_train_final), ('Validation', y_val_final), ('Test', y_test_final)]):\n",
    "    axes[i].hist(y_data, bins=20, alpha=0.7, edgecolor='black')\n",
    "    axes[i].set_title(f'{split_name} (n={len(y_data)})')\n",
    "    axes[i].set_xlabel('pKi/pKd/pIC50')\n",
    "    axes[i].set_ylabel('Frequency')\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add statistics\n",
    "    mean_val = y_data.mean()\n",
    "    std_val = y_data.std()\n",
    "    axes[i].axvline(mean_val, color='red', linestyle='--', alpha=0.8, label=f'Mean: {mean_val:.2f}')\n",
    "    axes[i].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n=== Target Variable Statistics ===\")\n",
    "for split_name, y_data in [('Train', y_train_final), ('Validation', y_val_final), ('Test', y_test_final)]:\n",
    "    print(f\"{split_name}: mean={y_data.mean():.2f}, std={y_data.std():.2f}, min={y_data.min():.2f}, max={y_data.max():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a291ed97",
   "metadata": {},
   "source": [
    "## 8. Feature Engineering for Pose-Free Prediction\n",
    "\n",
    "Now we'll combine protein and ligand features into input vectors suitable for machine learning models. This pose-free approach avoids the need for 3D structural information and focuses on sequence-based and chemical descriptors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa199b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine protein and molecular features for pose-free prediction\n",
    "def create_combined_features(df_split, protein_features_df, mol_features_df):\n",
    "    \"\"\"\n",
    "    Combine protein and molecular features for each protein-ligand pair\n",
    "    \"\"\"\n",
    "    combined_features = []\n",
    "    \n",
    "    for idx, row in df_split.iterrows():\n",
    "        uniprot_id = row['UniProt (SwissProt) Primary ID of Target Chain']\n",
    "        smiles = row['Ligand SMILES']\n",
    "        \n",
    "        # Get protein features\n",
    "        protein_row = protein_features_df[protein_features_df['uniprot_id'] == uniprot_id]\n",
    "        if len(protein_row) == 0:\n",
    "            # Use default protein features if not found\n",
    "            protein_feats = {\n",
    "                'sequence_length': 300, 'molecular_weight': 35000, 'aromaticity': 0.1,\n",
    "                'instability_index': 40, 'isoelectric_point': 7.0, 'gravy': -0.5,\n",
    "                'helix_fraction': 0.3, 'turn_fraction': 0.3, 'sheet_fraction': 0.4\n",
    "            }\n",
    "        else:\n",
    "            protein_feats = protein_row.iloc[0].to_dict()\n",
    "            del protein_feats['uniprot_id']  # Remove ID column\n",
    "        \n",
    "        # Get molecular features\n",
    "        mol_row = mol_features_df[mol_features_df['smiles'] == smiles]\n",
    "        if len(mol_row) == 0:\n",
    "            # Use default molecular features if not found\n",
    "            mol_feats = {\n",
    "                'mol_weight': 300, 'logp': 2.0, 'num_h_donors': 2, 'num_h_acceptors': 4,\n",
    "                'num_rotatable_bonds': 3, 'tpsa': 60, 'num_aromatic_rings': 1,\n",
    "                'num_aliphatic_rings': 0, 'num_heteroatoms': 3, 'num_heavy_atoms': 20\n",
    "            }\n",
    "        else:\n",
    "            mol_feats = mol_row.iloc[0].to_dict()\n",
    "            del mol_feats['smiles']  # Remove SMILES column\n",
    "        \n",
    "        # Combine features\n",
    "        combined_feats = {**protein_feats, **mol_feats}\n",
    "        \n",
    "        # Add interaction features (simple examples)\n",
    "        combined_feats['protein_mol_mw_ratio'] = protein_feats.get('molecular_weight', 35000) / mol_feats.get('mol_weight', 300)\n",
    "        combined_feats['hydrophobic_match'] = protein_feats.get('gravy', -0.5) * mol_feats.get('logp', 2.0)\n",
    "        \n",
    "        combined_features.append(combined_feats)\n",
    "    \n",
    "    return pd.DataFrame(combined_features)\n",
    "\n",
    "# Create combined feature matrices for each split\n",
    "print(\"Creating combined feature matrices...\")\n",
    "\n",
    "if 'protein_features_df' in globals() and 'mol_features_df' in globals():\n",
    "    X_train = create_combined_features(df_train_final, protein_features_df, mol_features_df)\n",
    "    X_val = create_combined_features(df_val_final, protein_features_df, mol_features_df)\n",
    "    X_test = create_combined_features(df_test_final, protein_features_df, mol_features_df)\n",
    "    \n",
    "    print(f\"✓ Created feature matrices:\")\n",
    "    print(f\"  Train: {X_train.shape}\")\n",
    "    print(f\"  Validation: {X_val.shape}\")\n",
    "    print(f\"  Test: {X_test.shape}\")\n",
    "    \n",
    "    # Display feature summary\n",
    "    print(f\"\\nTotal features: {X_train.shape[1]}\")\n",
    "    print(\"\\nSample features:\")\n",
    "    feature_sample = X_train.head()\n",
    "    display(feature_sample.iloc[:, :10])  # Show first 10 features\n",
    "    \n",
    "    # Scale features for machine learning\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train.fillna(0))\n",
    "    X_val_scaled = scaler.transform(X_val.fillna(0))\n",
    "    X_test_scaled = scaler.transform(X_test.fillna(0))\n",
    "    \n",
    "    print(f\"\\n✓ Features scaled and ready for machine learning\")\n",
    "    print(f\"Feature matrix shapes after scaling:\")\n",
    "    print(f\"  X_train: {X_train_scaled.shape}\")\n",
    "    print(f\"  X_val: {X_val_scaled.shape}\") \n",
    "    print(f\"  X_test: {X_test_scaled.shape}\")\n",
    "    \n",
    "    # Feature importance analysis (correlation with target)\n",
    "    feature_correlations = []\n",
    "    for i, feature_name in enumerate(X_train.columns):\n",
    "        corr = np.corrcoef(X_train_scaled[:, i], y_train_final)[0, 1]\n",
    "        if not np.isnan(corr):\n",
    "            feature_correlations.append((feature_name, abs(corr)))\n",
    "    \n",
    "    # Sort by absolute correlation\n",
    "    feature_correlations.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    print(f\"\\n=== Top 10 Features by Correlation with Binding Affinity ===\")\n",
    "    for i, (feature, corr) in enumerate(feature_correlations[:10]):\n",
    "        print(f\"{i+1:2d}. {feature}: {corr:.3f}\")\n",
    "    \n",
    "else:\n",
    "    print(\"❌ Protein or molecular features not available\")\n",
    "    print(\"Create sample feature matrices for demonstration...\")\n",
    "    \n",
    "    # Create minimal feature matrices for demo\n",
    "    n_protein_features = 20\n",
    "    n_molecular_features = 30\n",
    "    \n",
    "    X_train = pd.DataFrame(\n",
    "        np.random.randn(len(df_train_final), n_protein_features + n_molecular_features),\n",
    "        columns=[f'feature_{i}' for i in range(n_protein_features + n_molecular_features)]\n",
    "    )\n",
    "    X_val = pd.DataFrame(\n",
    "        np.random.randn(len(df_val_final), n_protein_features + n_molecular_features),\n",
    "        columns=[f'feature_{i}' for i in range(n_protein_features + n_molecular_features)]\n",
    "    )\n",
    "    X_test = pd.DataFrame(\n",
    "        np.random.randn(len(df_test_final), n_protein_features + n_molecular_features),\n",
    "        columns=[f'feature_{i}' for i in range(n_protein_features + n_molecular_features)]\n",
    "    )\n",
    "    \n",
    "    print(f\"Created demo feature matrices: {X_train.shape[1]} features\")\n",
    "\n",
    "print(f\"\\n🎯 Dataset ready for machine learning!\")\n",
    "print(f\"📊 Summary:\")\n",
    "print(f\"   • Total samples: {len(df_train_final) + len(df_val_final) + len(df_test_final)}\")\n",
    "print(f\"   • Features: {X_train.shape[1] if 'X_train' in locals() else 'N/A'}\")\n",
    "print(f\"   • Target: pKi/pKd/pIC50 (continuous)\")\n",
    "print(f\"   • Model constraint: ≤50M parameters\")\n",
    "print(f\"   • Approach: Pose-free binding affinity prediction\")\n",
    "\n",
    "print(f\"\\n📝 Next steps for hackathon:\")\n",
    "print(f\"   1. Train lightweight ML models (Random Forest, XGBoost, small NN)\")\n",
    "print(f\"   2. Implement protein language model embeddings (ESM-2)\")\n",
    "print(f\"   3. Add molecular fingerprints and graph neural networks\")\n",
    "print(f\"   4. Create ensemble models for robust predictions\")\n",
    "print(f\"   5. Build interactive demo interface\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
